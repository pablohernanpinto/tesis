{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Ejemplo de datos con 3 características y 2 clases\n",
    "data = {\n",
    "    'feature1': np.random.rand(100),\n",
    "    'feature2': np.random.rand(100),\n",
    "    'feature3': np.random.rand(100),\n",
    "    'label': np.random.choice([0, 1], size=100)\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = df[['feature1', 'feature2', 'feature3']].values\n",
    "y = df['label'].values\n",
    "\n",
    "X_scaled = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LeakyReLU, BatchNormalization\n",
    "\n",
    "def create_generator(input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=input_dim))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(128))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(3, activation='tanh'))  # Tres características generadas\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_discriminator(input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_dim=input_dim))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Dense(64))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Salida binaria: 0 o 1\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import Input\n",
    "\n",
    "def train_gan(generator, discriminator, epochs, batch_size, X_real):\n",
    "    # Configuración de optimizadores\n",
    "    adam = Adam(lr=0.0002, beta_1=0.5)\n",
    "\n",
    "    # Discriminador\n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "    # Generador\n",
    "    discriminator.trainable = False\n",
    "    z = Input(shape=(100,))  # Ruido de entrada\n",
    "    generated_data = generator(z)\n",
    "    validity = discriminator(generated_data)\n",
    "    combined = Model(z, validity)\n",
    "    combined.compile(loss='binary_crossentropy', optimizer=adam)\n",
    "\n",
    "    # Entrenamiento\n",
    "    for epoch in range(epochs):\n",
    "        # Entrenamiento del discriminador\n",
    "        idx = np.random.randint(0, X_real.shape[0], batch_size)\n",
    "        real_data = X_real[idx]\n",
    "        noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "        generated_data = generator.predict(noise)\n",
    "        \n",
    "        # Entrenamiento del discriminador (real = 1, generado = 0)\n",
    "        d_loss_real = discriminator.train_on_batch(real_data, np.ones((batch_size, 1)))\n",
    "        d_loss_fake = discriminator.train_on_batch(generated_data, np.zeros((batch_size, 1)))\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "        # Entrenamiento del generador\n",
    "        noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "        g_loss = combined.train_on_batch(noise, np.ones((batch_size, 1)))  # El generador quiere engañar al discriminador\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"{epoch} [D loss: {d_loss[0]} | D accuracy: {100*d_loss[1]}] [G loss: {g_loss}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m noise \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, (\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m100\u001b[39m))  \u001b[38;5;66;03m# Generar 10 muestras\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m generated_data \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(noise)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Desnormalizar si es necesario\u001b[39;00m\n\u001b[1;32m      5\u001b[0m generated_data \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39minverse_transform(generated_data)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generator' is not defined"
     ]
    }
   ],
   "source": [
    "noise = np.random.normal(0, 1, (10, 100))  # Generar 10 muestras\n",
    "generated_data = generator.predict(noise)\n",
    "\n",
    "# Desnormalizar si es necesario\n",
    "generated_data = scaler.inverse_transform(generated_data)\n",
    "\n",
    "# Mostrar los datos generados\n",
    "print(generated_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LeakyReLU, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-19 18:48:21.999983: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-19 18:48:22.654631: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 950 MB memory:  -> device: 0, name: NVIDIA RTX A4000, pci bus id: 0000:c3:00.0, compute capability: 8.6\n",
      "/home/hpinto/.local/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Input' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 51\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Paso 7: Crear el modelo combinado (generador + discriminador)\u001b[39;00m\n\u001b[1;32m     50\u001b[0m discriminator\u001b[38;5;241m.\u001b[39mtrainable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[43mInput\u001b[49m(shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m100\u001b[39m,))  \u001b[38;5;66;03m# Ruido de entrada\u001b[39;00m\n\u001b[1;32m     52\u001b[0m generated_data \u001b[38;5;241m=\u001b[39m generator(z)\n\u001b[1;32m     53\u001b[0m validity \u001b[38;5;241m=\u001b[39m discriminator(generated_data)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Input' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Paso 1: Preprocesar datos\n",
    "# Ejemplo de datos con 3 características y 2 clases\n",
    "data = {\n",
    "    'feature1': np.random.rand(100),\n",
    "    'feature2': np.random.rand(100),\n",
    "    'feature3': np.random.rand(100),\n",
    "    'label': np.random.choice([0, 1], size=100)\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Normalizar los datos de características\n",
    "scaler = StandardScaler()\n",
    "X = df[['feature1', 'feature2', 'feature3']].values\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Paso 2: Definir el modelo generador\n",
    "def create_generator(input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=input_dim))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(128))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(3, activation='tanh'))  # Tres características generadas\n",
    "    return model\n",
    "\n",
    "# Paso 3: Definir el modelo discriminador\n",
    "def create_discriminator(input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_dim=input_dim))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Dense(64))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Salida binaria: 0 o 1\n",
    "    return model\n",
    "\n",
    "# Paso 4: Crear el generador y el discriminador\n",
    "generator = create_generator(100)  # 100 es el tamaño del vector de ruido\n",
    "discriminator = create_discriminator(3)  # 3 características generadas\n",
    "\n",
    "# Paso 5: Configurar el optimizador\n",
    "adam = Adam(lr=0.0002, beta_1=0.5)\n",
    "\n",
    "# Paso 6: Compilar el discriminador\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "# Paso 7: Crear el modelo combinado (generador + discriminador)\n",
    "discriminator.trainable = False\n",
    "z = Input(shape=(100,))  # Ruido de entrada\n",
    "generated_data = generator(z)\n",
    "validity = discriminator(generated_data)\n",
    "combined = Model(z, validity)\n",
    "combined.compile(loss='binary_crossentropy', optimizer=adam)\n",
    "\n",
    "# Paso 8: Generar datos\n",
    "noise = np.random.normal(0, 1, (10, 100))  # Generar 10 muestras de ruido\n",
    "generated_data = generator.predict(noise)\n",
    "\n",
    "# Desnormalizar los datos generados\n",
    "generated_data = scaler.inverse_transform(generated_data)\n",
    "\n",
    "# Mostrar los datos generados\n",
    "print(generated_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
