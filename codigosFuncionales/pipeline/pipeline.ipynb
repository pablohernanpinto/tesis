{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_coli_driams_b_2000_20000Da_v2 (1).csv\n",
      "Archivo: e_coli_driams_b_2000_20000Da_v2 (1).csv Bacteria: Cefepime\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hpinto/.local/lib/python3.9/site-packages/sdv/single_table/base.py:119: FutureWarning: The 'SingleTableMetadata' is deprecated. Please use the new 'Metadata' class for synthesizers.\n",
      "  warnings.warn(DEPRECATION_MSG, FutureWarning)\n",
      "/home/hpinto/.local/lib/python3.9/site-packages/sdv/single_table/base.py:104: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Modelo_s_aureus_ciprofloxacin\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Conv_1 (Conv1D)             (None, 5984, 64)          1152      \n",
      "                                                                 \n",
      " batch_normalization_40 (Bat  (None, 5984, 64)         256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_40 (Activation)  (None, 5984, 64)          0         \n",
      "                                                                 \n",
      " MaxPooling1D_1 (MaxPooling1  (None, 2992, 64)         0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " Conv_2 (Conv1D)             (None, 2984, 128)         73856     \n",
      "                                                                 \n",
      " batch_normalization_41 (Bat  (None, 2984, 128)        512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_41 (Activation)  (None, 2984, 128)         0         \n",
      "                                                                 \n",
      " MaxPooling1D_2 (MaxPooling1  (None, 1492, 128)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " Conv_3 (Conv1D)             (None, 1488, 256)         164096    \n",
      "                                                                 \n",
      " batch_normalization_42 (Bat  (None, 1488, 256)        1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_42 (Activation)  (None, 1488, 256)         0         \n",
      "                                                                 \n",
      " MaxPooling1D_3 (MaxPooling1  (None, 744, 256)         0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " Conv_4 (Conv1D)             (None, 740, 256)          327936    \n",
      "                                                                 \n",
      " batch_normalization_43 (Bat  (None, 740, 256)         1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_43 (Activation)  (None, 740, 256)          0         \n",
      "                                                                 \n",
      " MaxPooling1D_4 (MaxPooling1  (None, 370, 256)         0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " flatten_10 (Flatten)        (None, 94720)             0         \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 94720)             0         \n",
      "                                                                 \n",
      " fully_connected_0 (Dense)   (None, 256)               24248576  \n",
      "                                                                 \n",
      " fully_connected_1 (Dense)   (None, 64)                16448     \n",
      "                                                                 \n",
      " fully_connected_2 (Dense)   (None, 64)                4160      \n",
      "                                                                 \n",
      " OUT_Layer (Dense)           (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24,839,105\n",
      "Trainable params: 24,837,697\n",
      "Non-trainable params: 1,408\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "25/25 [==============================] - 2s 46ms/step - loss: 7.1848 - tp: 55.0000 - fp: 47.0000 - tn: 89.0000 - fn: 53.0000 - accuracy: 0.5902 - precision: 0.5392 - recall: 0.5093 - auc: 0.5921 - prc: 0.5191 - val_loss: 6.8726 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 0.0000e+00 - val_fn: 28.0000 - val_accuracy: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.0000e+00 - val_prc: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 2/100\n",
      "25/25 [==============================] - 1s 23ms/step - loss: 6.7331 - tp: 61.0000 - fp: 37.0000 - tn: 99.0000 - fn: 47.0000 - accuracy: 0.6557 - precision: 0.6224 - recall: 0.5648 - auc: 0.6863 - prc: 0.6594 - val_loss: 6.9029 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 0.0000e+00 - val_fn: 28.0000 - val_accuracy: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.0000e+00 - val_prc: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 3/100\n",
      "25/25 [==============================] - 1s 27ms/step - loss: 6.6160 - tp: 74.0000 - fp: 33.0000 - tn: 103.0000 - fn: 34.0000 - accuracy: 0.7254 - precision: 0.6916 - recall: 0.6852 - auc: 0.7779 - prc: 0.7500 - val_loss: 6.8543 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 0.0000e+00 - val_fn: 28.0000 - val_accuracy: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.0000e+00 - val_prc: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 4/100\n",
      "25/25 [==============================] - 1s 25ms/step - loss: 6.4541 - tp: 79.0000 - fp: 22.0000 - tn: 114.0000 - fn: 29.0000 - accuracy: 0.7910 - precision: 0.7822 - recall: 0.7315 - auc: 0.8369 - prc: 0.8121 - val_loss: 6.9339 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 0.0000e+00 - val_fn: 28.0000 - val_accuracy: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.0000e+00 - val_prc: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 5/100\n",
      "25/25 [==============================] - 1s 26ms/step - loss: 6.3460 - tp: 73.0000 - fp: 19.0000 - tn: 117.0000 - fn: 35.0000 - accuracy: 0.7787 - precision: 0.7935 - recall: 0.6759 - auc: 0.8609 - prc: 0.8254 - val_loss: 6.7766 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 0.0000e+00 - val_fn: 28.0000 - val_accuracy: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.0000e+00 - val_prc: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 6/100\n",
      "25/25 [==============================] - 1s 24ms/step - loss: 6.3208 - tp: 77.0000 - fp: 21.0000 - tn: 115.0000 - fn: 31.0000 - accuracy: 0.7869 - precision: 0.7857 - recall: 0.7130 - auc: 0.8289 - prc: 0.8414 - val_loss: 6.8817 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 0.0000e+00 - val_fn: 28.0000 - val_accuracy: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.0000e+00 - val_prc: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 7/100\n",
      "25/25 [==============================] - 1s 24ms/step - loss: 6.1877 - tp: 78.0000 - fp: 18.0000 - tn: 118.0000 - fn: 30.0000 - accuracy: 0.8033 - precision: 0.8125 - recall: 0.7222 - auc: 0.8716 - prc: 0.8753 - val_loss: 6.8610 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 0.0000e+00 - val_fn: 28.0000 - val_accuracy: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.0000e+00 - val_prc: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 8/100\n",
      "25/25 [==============================] - 1s 24ms/step - loss: 6.0807 - tp: 84.0000 - fp: 15.0000 - tn: 121.0000 - fn: 24.0000 - accuracy: 0.8402 - precision: 0.8485 - recall: 0.7778 - auc: 0.9052 - prc: 0.8975 - val_loss: 7.0036 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 0.0000e+00 - val_fn: 28.0000 - val_accuracy: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.0000e+00 - val_prc: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 9/100\n",
      "25/25 [==============================] - 1s 26ms/step - loss: 6.0148 - tp: 77.0000 - fp: 5.0000 - tn: 131.0000 - fn: 31.0000 - accuracy: 0.8525 - precision: 0.9390 - recall: 0.7130 - auc: 0.9298 - prc: 0.9336 - val_loss: 6.9256 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 0.0000e+00 - val_fn: 28.0000 - val_accuracy: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.0000e+00 - val_prc: 1.0000 - lr: 1.0000e-05\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "28/28 [==============================] - 0s 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hpinto/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/hpinto/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/hpinto/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 4ms/step\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  # Deshabilitar GPU\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,classification_report,ConfusionMatrixDisplay, balanced_accuracy_score\n",
    "#from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler, MinMaxScaler, MaxAbsScaler, RobustScaler, QuantileTransformer, PowerTransformer\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import regularizers\n",
    "from keras.optimizers import Adam\n",
    "#from keras.backend import expand_dims\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from keras.models import Sequential,Model\n",
    "from keras.constraints import MaxNorm\n",
    "from keras.layers import Activation, Dense, Conv1D, Flatten, MaxPooling1D, Dropout, BatchNormalization, SpatialDropout1D,Lambda,Input\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import gc\n",
    "from tensorflow.keras.losses import mse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sdv.metadata import SingleTableMetadata\n",
    "from sdv.single_table import GaussianCopulaSynthesizer\n",
    "\n",
    "# Deshabilitar GPU en TensorFlow\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "METRICS = [\n",
    "      keras.metrics.TruePositives(name='tp'),\n",
    "      keras.metrics.FalsePositives(name='fp'),\n",
    "      keras.metrics.TrueNegatives(name='tn'),\n",
    "      keras.metrics.FalseNegatives(name='fn'),\n",
    "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall'),\n",
    "      keras.metrics.AUC(name='auc'),\n",
    "      keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "]\n",
    "\n",
    "def Crear_modelo(X_train_reshaped,y_train):\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, min_lr=0.000001)\n",
    "    early_st = EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)\n",
    "\n",
    "    n_timesteps = X_train_reshaped.shape[1] #\n",
    "    n_features  = X_train_reshaped.shape[2] #\n",
    "\n",
    "    model = Sequential(name=\"Modelo_s_aureus_ciprofloxacin\")\n",
    "    init_mode = 'normal'\n",
    "    model.add(Conv1D(filters=(64), kernel_size=(17), input_shape = (n_timesteps,n_features), name='Conv_1'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2, name=\"MaxPooling1D_1\"))\n",
    "\n",
    "    model.add(Conv1D(filters=(128), kernel_size=(9),kernel_initializer=init_mode, kernel_regularizer=regularizers.l2(0.0001),  name='Conv_2'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2, name=\"MaxPooling1D_2\"))\n",
    "\n",
    "    model.add(Conv1D(filters=(256), kernel_size=(5),kernel_initializer=init_mode,kernel_regularizer=regularizers.l2(0.0001),   name='Conv_3'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2, name=\"MaxPooling1D_3\"))\n",
    "\n",
    "    model.add(Conv1D(filters=(256), kernel_size=(5),kernel_initializer=init_mode, kernel_regularizer=regularizers.l2(0.0001),   name='Conv_4'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2, name=\"MaxPooling1D_4\"))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.65))\n",
    "    model.add(Dense(256, activation='relu',kernel_initializer=init_mode, kernel_regularizer=regularizers.l2(0.0001), name=\"fully_connected_0\"))\n",
    "    model.add(Dense(64, activation='relu',kernel_initializer=init_mode, kernel_regularizer=regularizers.l2(0.0001), name=\"fully_connected_1\"))\n",
    "    model.add(Dense(64, activation='relu',kernel_initializer=init_mode, kernel_regularizer=regularizers.l2(0.0001),  name=\"fully_connected_2\"))\n",
    "    model.add(Dense(n_features, activation='sigmoid', name=\"OUT_Layer\"))\n",
    "\n",
    "    model.compile(optimizer = Adam(learning_rate=0.0001), loss = 'binary_crossentropy',  metrics=METRICS)\n",
    "    model.summary()\n",
    "    history = model.fit(X_train_reshaped, y_train, epochs=100, batch_size=10, verbose=1, validation_split=0.1, callbacks=[reduce_lr,early_st])\n",
    "    return model\n",
    "\n",
    "def normalizacion(X_train, X_test):\n",
    "    scaler=Normalizer(norm='max')\n",
    "    sc_X = scaler\n",
    "    X_train = sc_X.fit_transform(X_train)\n",
    "    X_test = sc_X.transform(X_test)\n",
    "\n",
    "    sample_size = X_train.shape[0] # numero de muestras en el set de datos\n",
    "    time_steps  = X_train.shape[1] # numero de atributos en el set de datos\n",
    "    input_dimension = 1            #\n",
    "\n",
    "    X_train_reshaped = X_train.reshape(sample_size,time_steps,input_dimension)\n",
    "    X_test_reshaped = X_test.reshape(X_test.shape[0],X_test.shape[1],1)\n",
    "    return X_train_reshaped,X_test_reshaped\n",
    "\n",
    "\n",
    "def entrenamiento_base(X_train, X_test, y_train, y_test):\n",
    "    X_train_reshaped,X_test_reshaped = normalizacion(X_train, X_test)\n",
    "    \n",
    "    model = Crear_modelo(X_train_reshaped,y_train)\n",
    "\n",
    "    y_pred  = model.predict(X_test_reshaped)\n",
    "    y_pred = (y_pred>0.5)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    #model,tipo_entrenamiento,cm,y_pred,X_test_reshaped,X_train_reshaped\n",
    "    return model,'Entrenamiento base',cm,y_pred,X_test_reshaped,X_train_reshaped\n",
    "\n",
    "def Aplicar_Smote(X_train, X_test, y_train, y_test):\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_resampled_smote, y_resampled_smote = smote.fit_resample(X_train, y_train)\n",
    "    X_train_reshaped,X_test_reshaped = normalizacion(X_resampled_smote, X_test)\n",
    "\n",
    "    model = Crear_modelo(X_train_reshaped,y_resampled_smote)\n",
    "    y_pred  = model.predict(X_test_reshaped)\n",
    "\n",
    "    y_pred = (y_pred>0.5)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    #model,tipo_entrenamiento,cm,y_pred,X_test_reshaped,X_train_reshaped\n",
    "    return model,'Entrenamiento Smote',cm,y_pred,X_test_reshaped,X_train_reshaped\n",
    "\n",
    "def Aplicar_VAE(df_bacteria,bacteria,X_train, X_test, y_train, y_test):\n",
    "    minority_class = df_bacteria[df_bacteria[bacteria] == 1].drop(columns=[bacteria])\n",
    "    scaler = MinMaxScaler()\n",
    "    X_minority_scaled = scaler.fit_transform(minority_class)\n",
    "    # Dimensiones\n",
    "    input_dim = X_minority_scaled.shape[1]\n",
    "    latent_dim = 2  # Espacio latente\n",
    "\n",
    "    # Encoder\n",
    "    inputs = Input(shape=(input_dim,))\n",
    "    hidden = Dense(16, activation='relu')(inputs)\n",
    "    z_mean = Dense(latent_dim, name='z_mean')(hidden)\n",
    "    z_log_var = Dense(latent_dim, name='z_log_var')(hidden)\n",
    "\n",
    "    # Sampling\n",
    "    def sampling(args):\n",
    "        z_mean, z_log_var = args\n",
    "        epsilon = tf.random.normal(shape=(tf.shape(z_mean)[0], latent_dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "    z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "\n",
    "    # Decoder\n",
    "    decoder_hidden = Dense(16, activation='relu')\n",
    "    decoder_output = Dense(input_dim, activation='sigmoid')\n",
    "\n",
    "    hidden_decoded = decoder_hidden(z)\n",
    "    outputs = decoder_output(hidden_decoded)\n",
    "\n",
    "    # Modelo VAE\n",
    "    vae = Model(inputs, outputs)\n",
    "\n",
    "    # Pérdida personalizada\n",
    "    reconstruction_loss = mse(inputs, outputs)\n",
    "    reconstruction_loss *= input_dim\n",
    "    kl_loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n",
    "    kl_loss = tf.reduce_sum(kl_loss, axis=-1)\n",
    "    kl_loss *= -0.5\n",
    "    vae_loss = tf.reduce_mean(reconstruction_loss + kl_loss)\n",
    "    vae.add_loss(vae_loss)\n",
    "    vae.compile(optimizer='adam')\n",
    "\n",
    "    vae.summary()\n",
    "    vae.fit(X_minority_scaled, X_minority_scaled, epochs=200, batch_size=32, verbose=1)\n",
    "    # Construir el generador (Decoder independiente)\n",
    "    decoder_input = Input(shape=(latent_dim,))\n",
    "    hidden_decoded_2 = decoder_hidden(decoder_input)\n",
    "    output_decoded = decoder_output(hidden_decoded_2)\n",
    "    generator = Model(decoder_input, output_decoded)\n",
    "\n",
    "    # Generar datos sintéticos\n",
    "    print(pd.Series(y_train).value_counts())\n",
    "    num_samples = pd.Series(y_train).value_counts()[0]-pd.Series(y_train).value_counts()[1]\n",
    "    latent_points = np.random.normal(size=(num_samples, latent_dim))\n",
    "    synthetic_data = generator.predict(latent_points)\n",
    "\n",
    "\n",
    "    # Escalar de vuelta a los valores originales\n",
    "    synthetic_data_original = scaler.inverse_transform(synthetic_data)\n",
    "    X_train_balanced = np.concatenate([X_train, synthetic_data_original])\n",
    "    y_train_balanced = np.concatenate([y_train, np.ones(num_samples)])\n",
    "\n",
    "    X_train_reshaped,X_test_reshaped = normalizacion(X_train_balanced, X_test)\n",
    "    \n",
    "    model = Crear_modelo(X_train_reshaped,y_train_balanced)\n",
    "\n",
    "    y_pred  = model.predict(X_test_reshaped)\n",
    "    y_pred = (y_pred>0.5)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    #model,tipo_entrenamiento,cm,y_pred,X_test_reshaped,X_train_reshaped\n",
    "    return model,'Entrenamiento VAE',cm,y_pred,X_test_reshaped,X_train_reshaped\n",
    "\n",
    "\n",
    "def Aplicar_DifussionModel(df_bacteria,bacteria,X_train, X_test, y_train, y_test):\n",
    "    # Preprocesamiento\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(X_train)\n",
    "    # Modelo de Difusión\n",
    "    class DiffusionModel(nn.Module):\n",
    "        def __init__(self, input_dim):\n",
    "            super(DiffusionModel, self).__init__()\n",
    "            self.model = nn.Sequential( \n",
    "                nn.Linear(input_dim, 64),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(p=0.2),  # Regularización Dropout\n",
    "                nn.Linear(64, 32),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(p=0.2),  # Regularización Dropout\n",
    "                nn.Linear(32, input_dim)\n",
    "            )\n",
    "        def forward(self, x):\n",
    "            return self.model(x)\n",
    "    # Función de ruido (Scheduler)\n",
    "    def add_noise(data, timesteps, noise_scale=1.0):\n",
    "        noise = np.random.normal(0, noise_scale, data.shape) * np.sqrt(timesteps / 100)\n",
    "        noisy_data = data + noise\n",
    "        return noisy_data, noise\n",
    "    \n",
    "    # Configuración del modelo\n",
    "    input_dim = scaled_data.shape[1]\n",
    "    model = DiffusionModel(input_dim)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_fn = nn.SmoothL1Loss()  # O Huber Loss\n",
    "\n",
    "    # Scheduler de tasa de aprendizaje\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.9)\n",
    "\n",
    "    # Entrenamiento\n",
    "    scaled_data_tensor = torch.tensor(scaled_data, dtype=torch.float32)\n",
    "    epochs = 1000\n",
    "    losses = []  # Para guardar la pérdida por época\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        timesteps = np.random.randint(1, 100)\n",
    "        noisy_data, noise = add_noise(scaled_data, timesteps)\n",
    "        noisy_data_tensor = torch.tensor(noisy_data, dtype=torch.float32)\n",
    "        noise_tensor = torch.tensor(noise, dtype=torch.float32)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predicted_noise = model(noisy_data_tensor)\n",
    "        loss = loss_fn(predicted_noise, noise_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Actualiza la tasa de aprendizaje\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}/{epochs} - Loss: {loss.item()}\")\n",
    "\n",
    "        # Generación de Datos Sintéticos\n",
    "    def generate_synthetic_data(model, num_samples, input_dim):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            synthetic_data = np.random.normal(0, 1, (num_samples, input_dim))\n",
    "            for t in range(100, 0, -1):  # Reverse diffusion\n",
    "                synthetic_data = synthetic_data - model(torch.tensor(synthetic_data, dtype=torch.float32)).numpy() * (t / 100)\n",
    "            return synthetic_data\n",
    "        \n",
    "        \n",
    "    synthetic_data = generate_synthetic_data(model, pd.Series(y_train).value_counts()[0]-pd.Series(y_train).value_counts()[1], input_dim)\n",
    "    synthetic_data_rescaled = scaler.inverse_transform(synthetic_data)\n",
    "\n",
    "        \n",
    "    # Cambiar el tipo de datos a float32\n",
    "    synthetic_samples_numpy = synthetic_data_rescaled.astype(np.float32)\n",
    "\n",
    "    # Mostrar las muestras generadas\n",
    "    synthetic_samples_numpy.shape\n",
    "\n",
    "    X_train_resampled = np.concatenate([X_train,synthetic_samples_numpy])\n",
    "\n",
    "    ones_array = np.ones(pd.Series(y_train).value_counts()[0]-pd.Series(y_train).value_counts()[1])\n",
    "    y_train_resampled = np.concatenate([y_train,ones_array])\n",
    "\n",
    "    #termino de oversampling\n",
    "    X_train_reshaped,X_test_reshaped = normalizacion(X_train_resampled, X_test)\n",
    "    \n",
    "    model = Crear_modelo(X_train_reshaped,y_train_resampled)\n",
    "\n",
    "    y_pred  = model.predict(X_test_reshaped)\n",
    "    y_pred = (y_pred>0.5)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    #model,tipo_entrenamiento,cm,y_pred,X_test_reshaped,X_train_reshaped\n",
    "    return model,'Entrenamiento difussion model',cm,y_pred,X_test_reshaped,X_train_reshaped\n",
    "\n",
    "def Aplicar_Copulas(df_bacteria,bacteria,X_train, X_test, y_train, y_test):\n",
    "    minority_class = df_bacteria[df_bacteria[bacteria] == 1].drop(columns=[bacteria])\n",
    "\n",
    "    # Crear un objeto Metadata para el dataset\n",
    "    metadata = SingleTableMetadata()\n",
    "\n",
    "    # Detectar automáticamente los tipos de datos del DataFrame\n",
    "    metadata.detect_from_dataframe(minority_class)\n",
    "    synthesizer = GaussianCopulaSynthesizer(metadata)\n",
    "    synthesizer.fit(data=minority_class)\n",
    "\n",
    "    synthetic_data = synthesizer.sample(pd.Series(y_train).value_counts()[0]-pd.Series(y_train).value_counts()[1])\n",
    "    \n",
    "    # Cambiar el tipo de datos a float32\n",
    "    synthetic_samples_numpy_copula = synthetic_data.astype(np.float32)\n",
    "\n",
    "    # Mostrar las muestras generadas\n",
    "    synthetic_samples_numpy_copula.shape\n",
    "    X_train_resampled = np.concatenate([X_train,synthetic_samples_numpy_copula])\n",
    "\n",
    "    ones_array = np.ones(int((pd.Series(y_train).value_counts()[0]-pd.Series(y_train).value_counts()[1])))\n",
    "    y_train_resampled = np.concatenate([y_train,ones_array])\n",
    "\n",
    "\n",
    "    X_train_reshaped,X_test_reshaped = normalizacion(X_train_resampled, X_test)\n",
    "    \n",
    "    model = Crear_modelo(X_train_reshaped,y_train_resampled)\n",
    "\n",
    "    y_pred  = model.predict(X_test_reshaped)\n",
    "    y_pred = (y_pred>0.5)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    #model,tipo_entrenamiento,cm,y_pred,X_test_reshaped,X_train_reshaped\n",
    "    return model,'Entrenamiento Copulas',cm,y_pred,X_test_reshaped,X_train_reshaped\n",
    "\n",
    "\n",
    "def columnas_bacterias_fun(df):\n",
    "    vocales = ['a','e','i','o','u']\n",
    "    columnas_bacterias = []\n",
    "    for i in vocales:\n",
    "        for j in df.columns:\n",
    "            if i in j:\n",
    "                columnas_bacterias.append(j)\n",
    "    columnas_bacterias = list(set(columnas_bacterias))\n",
    "    return columnas_bacterias\n",
    "\n",
    "def inscripcion_resultados(model,archivo,bacteria,cm,y_test, y_pred,tipo_entrenamiento,X_test_reshaped,X_train_reshaped):\n",
    "    with open('resultados/resultados.txt', 'a') as archivo_:\n",
    "        # Redirige la salida estándar al archivo\n",
    "        print('-----------------------------------------------------\\n\\n','nombre de archivo:', archivo, '\\nBacteria:', bacteria, \"\\n\\nTipo de entrenamiento:\",tipo_entrenamiento,'\\n\\nconfusion_matrix:\\n', cm, file=archivo_)\n",
    "        target_names=[\"0\",\"1\"]\n",
    "        print('\\n\\n',classification_report(y_test, y_pred, target_names=target_names), file=archivo_)\n",
    "\n",
    "        train_predictions_baseline = model.predict(X_train_reshaped, batch_size=10)\n",
    "        test_predictions_baseline = model.predict(X_test_reshaped, batch_size=10)\n",
    "        print('\\n\\n')\n",
    "        baseline_results = model.evaluate(X_test_reshaped, y_test, verbose=0)\n",
    "        for name, value in zip(model.metrics_names, baseline_results):\n",
    "            print(name, ': ', value, file=archivo_)  \n",
    "\n",
    "\n",
    "files_list = os.listdir('SetDatos/')\n",
    "for archivo in ['e_coli_driams_b_2000_20000Da_v2 (1).csv']:#files_list\n",
    "    print(archivo)\n",
    "    df = pd.read_csv('SetDatos/'+archivo)\n",
    "    df = df.drop(columns=['code','species'])\n",
    "    df.dropna(axis=0, how=\"any\", inplace=True)\n",
    "    columnas_bacterias = columnas_bacterias_fun(df)\n",
    "    \n",
    "    for bacteria in ['Cefepime']: #columnas_bacterias\n",
    "\n",
    "        try:\n",
    "            print('Archivo:',archivo,'Bacteria:',bacteria)\n",
    "            columnas_bacterias_sin_bacteria = [b for b in columnas_bacterias if b != bacteria]\n",
    "            df_bacteria = df.drop(columns = columnas_bacterias_sin_bacteria)\n",
    "            bacteria = df_bacteria.columns[-1]\n",
    "            X = df_bacteria.iloc[:, 0:-1].values  # variables independientes (espectros de masa)\n",
    "            y = df_bacteria.iloc[:, -1].values    # variable dependientes (resistencia a ciprofloxacin)\n",
    "            X = np.asarray(X).astype(np.float32)\n",
    "            y = np.asarray(y).astype(np.float32)\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0, stratify=y)\n",
    " \n",
    "            #resultado sin oversampling\n",
    "            model,tipo_entrenamiento,cm,y_pred,X_test_reshaped,X_train_reshaped = entrenamiento_base(X_train, X_test, y_train, y_test)\n",
    "            inscripcion_resultados(model,archivo,bacteria,cm,y_test, y_pred,tipo_entrenamiento,X_test_reshaped,X_train_reshaped)\n",
    "        \n",
    "            # Liberar memoria\n",
    "            del model, tipo_entrenamiento, y_pred, cm, X_test_reshaped, X_train_reshaped\n",
    "            gc.collect()  # Forzar recolección de basura\n",
    "            \n",
    "            #resultado con smote\n",
    "            model,tipo_entrenamiento,cm,y_pred,X_test_reshaped,X_train_reshaped = Aplicar_Smote(X_train, X_test, y_train, y_test)\n",
    "            inscripcion_resultados(model,archivo,bacteria,cm,y_test, y_pred,tipo_entrenamiento,X_test_reshaped,X_train_reshaped)\n",
    "\n",
    "            # Liberar memoria\n",
    "            del model, tipo_entrenamiento, y_pred, cm, X_test_reshaped, X_train_reshaped\n",
    "            gc.collect()  # Forzar recolección de basura\n",
    "            # \n",
    "            #resultado con VAE\n",
    "            model,tipo_entrenamiento,cm,y_pred,X_test_reshaped,X_train_reshaped = Aplicar_VAE(df_bacteria,bacteria,X_train, X_test, y_train, y_test)\n",
    "            inscripcion_resultados(model,archivo,bacteria,cm,y_test, y_pred,tipo_entrenamiento,X_test_reshaped,X_train_reshaped)\n",
    "            \n",
    "            # Liberar memoria\n",
    "            del model, tipo_entrenamiento, y_pred, cm, X_test_reshaped, X_train_reshaped\n",
    "            gc.collect()  # Forzar recolección de basura\n",
    "\n",
    "            \n",
    "            #resultado con Difussion model\n",
    "            model,tipo_entrenamiento,cm,y_pred,X_test_reshaped,X_train_reshaped = Aplicar_DifussionModel(df_bacteria,bacteria,X_train, X_test, y_train, y_test)\n",
    "            inscripcion_resultados(model,archivo,bacteria,cm,y_test, y_pred,tipo_entrenamiento,X_test_reshaped,X_train_reshaped)\n",
    "            \n",
    "            # Liberar memoria\n",
    "            del model, tipo_entrenamiento, y_pred, cm, X_test_reshaped, X_train_reshaped\n",
    "            gc.collect()  # Forzar recolección de basura\n",
    "\n",
    "            #resultado con Difussion model\n",
    "            model,tipo_entrenamiento,cm,y_pred,X_test_reshaped,X_train_reshaped = Aplicar_Copulas(df_bacteria,bacteria,X_train, X_test, y_train, y_test)\n",
    "            inscripcion_resultados(model,archivo,bacteria,cm,y_test, y_pred,tipo_entrenamiento,X_test_reshaped,X_train_reshaped)\n",
    "            \n",
    "            # Liberar memoria\n",
    "            del model, tipo_entrenamiento, y_pred, cm, X_test_reshaped, X_train_reshaped\n",
    "            gc.collect()  # Forzar recolección de basura\n",
    "\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "            with open('resultados/resultados.txt', 'a') as archivo_:\n",
    "                print(\"Error:\",e,file = archivo_)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
