{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_coli_driams_b_2000_20000Da_v2 (1).csv\n",
      "Archivo: e_coli_driams_b_2000_20000Da_v2 (1).csv Bacteria: Ceftriaxone\n",
      "Model: \"model_17\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_18 (InputLayer)          [(None, 6000)]       0           []                               \n",
      "                                                                                                  \n",
      " dense_33 (Dense)               (None, 16)           96016       ['input_18[0][0]']               \n",
      "                                                                                                  \n",
      " z_mean (Dense)                 (None, 2)            34          ['dense_33[0][0]']               \n",
      "                                                                                                  \n",
      " z_log_var (Dense)              (None, 2)            34          ['dense_33[0][0]']               \n",
      "                                                                                                  \n",
      " z (Lambda)                     (None, 2)            0           ['z_mean[0][0]',                 \n",
      "                                                                  'z_log_var[0][0]']              \n",
      "                                                                                                  \n",
      " dense_34 (Dense)               (None, 16)           48          ['z[0][0]']                      \n",
      "                                                                                                  \n",
      " dense_35 (Dense)               (None, 6000)         102000      ['dense_34[0][0]']               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_22 (TFOpL  (None, 2)           0           ['z_log_var[0][0]']              \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.math.square_11 (TFOpLambda)  (None, 2)           0           ['z_mean[0][0]']                 \n",
      "                                                                                                  \n",
      " tf.convert_to_tensor_11 (TFOpL  (None, 6000)        0           ['dense_35[0][0]']               \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.cast_11 (TFOpLambda)        (None, 6000)         0           ['input_18[0][0]']               \n",
      "                                                                                                  \n",
      " tf.math.subtract_22 (TFOpLambd  (None, 2)           0           ['tf.__operators__.add_22[0][0]',\n",
      " a)                                                               'tf.math.square_11[0][0]']      \n",
      "                                                                                                  \n",
      " tf.math.exp_11 (TFOpLambda)    (None, 2)            0           ['z_log_var[0][0]']              \n",
      "                                                                                                  \n",
      " tf.math.squared_difference_11   (None, 6000)        0           ['tf.convert_to_tensor_11[0][0]',\n",
      " (TFOpLambda)                                                     'tf.cast_11[0][0]']             \n",
      "                                                                                                  \n",
      " tf.math.subtract_23 (TFOpLambd  (None, 2)           0           ['tf.math.subtract_22[0][0]',    \n",
      " a)                                                               'tf.math.exp_11[0][0]']         \n",
      "                                                                                                  \n",
      " tf.math.reduce_mean_22 (TFOpLa  (None,)             0           ['tf.math.squared_difference_11[0\n",
      " mbda)                                                           ][0]']                           \n",
      "                                                                                                  \n",
      " tf.math.reduce_sum_11 (TFOpLam  (None,)             0           ['tf.math.subtract_23[0][0]']    \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " tf.math.multiply_22 (TFOpLambd  (None,)             0           ['tf.math.reduce_mean_22[0][0]'] \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.multiply_23 (TFOpLambd  (None,)             0           ['tf.math.reduce_sum_11[0][0]']  \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_23 (TFOpL  (None,)             0           ['tf.math.multiply_22[0][0]',    \n",
      " ambda)                                                           'tf.math.multiply_23[0][0]']    \n",
      "                                                                                                  \n",
      " tf.math.reduce_mean_23 (TFOpLa  ()                  0           ['tf.__operators__.add_23[0][0]']\n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " add_loss_11 (AddLoss)          ()                   0           ['tf.math.reduce_mean_23[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 198,132\n",
      "Trainable params: 198,132\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 751.1832\n",
      "Epoch 2/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 722.2775\n",
      "Epoch 3/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 738.3002\n",
      "Epoch 4/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 729.2770\n",
      "Epoch 5/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 676.9679\n",
      "Epoch 6/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 701.5872\n",
      "Epoch 7/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 715.8939\n",
      "Epoch 8/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 724.0785\n",
      "Epoch 9/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 662.3401\n",
      "Epoch 10/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 656.5579\n",
      "Epoch 11/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 687.2631\n",
      "Epoch 12/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 699.2749\n",
      "Epoch 13/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 654.1616\n",
      "Epoch 14/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 656.2402\n",
      "Epoch 15/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 571.8262\n",
      "Epoch 16/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 541.5071\n",
      "Epoch 17/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 610.5010\n",
      "Epoch 18/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 564.4274\n",
      "Epoch 19/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 535.8688\n",
      "Epoch 20/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 535.2739\n",
      "Epoch 21/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 456.1507\n",
      "Epoch 22/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 540.4745\n",
      "Epoch 23/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 542.6171\n",
      "Epoch 24/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 477.4822\n",
      "Epoch 25/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 501.6906\n",
      "Epoch 26/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 464.2422\n",
      "Epoch 27/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 439.9054\n",
      "Epoch 28/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 403.8066\n",
      "Epoch 29/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 442.5527\n",
      "Epoch 30/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 406.3580\n",
      "Epoch 31/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 384.1424\n",
      "Epoch 32/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 347.0760\n",
      "Epoch 33/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 355.4291\n",
      "Epoch 34/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 379.8674\n",
      "Epoch 35/200\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 405.6484\n",
      "Epoch 36/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 335.7016\n",
      "Epoch 37/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 341.2296\n",
      "Epoch 38/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 284.7425\n",
      "Epoch 39/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 328.8167\n",
      "Epoch 40/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 354.8933\n",
      "Epoch 41/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 315.1369\n",
      "Epoch 42/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 316.9547\n",
      "Epoch 43/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 286.1550\n",
      "Epoch 44/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 380.3765\n",
      "Epoch 45/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 311.0802\n",
      "Epoch 46/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 277.9579\n",
      "Epoch 47/200\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 276.5851\n",
      "Epoch 48/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 283.1130\n",
      "Epoch 49/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 305.8238\n",
      "Epoch 50/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 253.2515\n",
      "Epoch 51/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 277.9699\n",
      "Epoch 52/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 317.9926\n",
      "Epoch 53/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 236.3958\n",
      "Epoch 54/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 257.9263\n",
      "Epoch 55/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 220.9817\n",
      "Epoch 56/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 269.7178\n",
      "Epoch 57/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 221.5791\n",
      "Epoch 58/200\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 234.1108\n",
      "Epoch 59/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 320.5439\n",
      "Epoch 60/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 261.3484\n",
      "Epoch 61/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 219.8289\n",
      "Epoch 62/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 251.1693\n",
      "Epoch 63/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 251.2127\n",
      "Epoch 64/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 214.9971\n",
      "Epoch 65/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 293.2572\n",
      "Epoch 66/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 184.5481\n",
      "Epoch 67/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 227.4440\n",
      "Epoch 68/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 248.6773\n",
      "Epoch 69/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 232.5452\n",
      "Epoch 70/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 222.1135\n",
      "Epoch 71/200\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 277.7141\n",
      "Epoch 72/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 219.6009\n",
      "Epoch 73/200\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 178.5734\n",
      "Epoch 74/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 243.3287\n",
      "Epoch 75/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 178.1200\n",
      "Epoch 76/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 263.9084\n",
      "Epoch 77/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 285.9383\n",
      "Epoch 78/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 236.8935\n",
      "Epoch 79/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 163.8445\n",
      "Epoch 80/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 199.3661\n",
      "Epoch 81/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 234.6115\n",
      "Epoch 82/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 186.6064\n",
      "Epoch 83/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 203.2129\n",
      "Epoch 84/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 222.8488\n",
      "Epoch 85/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 214.0549\n",
      "Epoch 86/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 171.0989\n",
      "Epoch 87/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 218.2557\n",
      "Epoch 88/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 164.1603\n",
      "Epoch 89/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 173.7645\n",
      "Epoch 90/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 150.6739\n",
      "Epoch 91/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 179.5570\n",
      "Epoch 92/200\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 166.0740\n",
      "Epoch 93/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 230.0384\n",
      "Epoch 94/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 165.0221\n",
      "Epoch 95/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 213.9641\n",
      "Epoch 96/200\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 192.3963\n",
      "Epoch 97/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 178.8494\n",
      "Epoch 98/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 239.8105\n",
      "Epoch 99/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 160.8360\n",
      "Epoch 100/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 208.5699\n",
      "Epoch 101/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 194.4395\n",
      "Epoch 102/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 205.3343\n",
      "Epoch 103/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 171.3804\n",
      "Epoch 104/200\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 182.3017\n",
      "Epoch 105/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 169.1302\n",
      "Epoch 106/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 208.1436\n",
      "Epoch 107/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 167.9559\n",
      "Epoch 108/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 194.4059\n",
      "Epoch 109/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 154.2916\n",
      "Epoch 110/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 167.1562\n",
      "Epoch 111/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 209.7555\n",
      "Epoch 112/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 149.9512\n",
      "Epoch 113/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 152.0483\n",
      "Epoch 114/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 174.1168\n",
      "Epoch 115/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 145.3203\n",
      "Epoch 116/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 178.5523\n",
      "Epoch 117/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 262.8394\n",
      "Epoch 118/200\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 166.7794\n",
      "Epoch 119/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 162.1882\n",
      "Epoch 120/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 221.8067\n",
      "Epoch 121/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 142.4748\n",
      "Epoch 122/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 120.5207\n",
      "Epoch 123/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 163.8745\n",
      "Epoch 124/200\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 161.9555\n",
      "Epoch 125/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 194.1441\n",
      "Epoch 126/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 188.7483\n",
      "Epoch 127/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 166.5292\n",
      "Epoch 128/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 196.3492\n",
      "Epoch 129/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 179.0075\n",
      "Epoch 130/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 218.5399\n",
      "Epoch 131/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 171.1902\n",
      "Epoch 132/200\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 234.1151\n",
      "Epoch 133/200\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 176.7596\n",
      "Epoch 134/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 157.4069\n",
      "Epoch 135/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 126.0343\n",
      "Epoch 136/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 157.1098\n",
      "Epoch 137/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 138.8897\n",
      "Epoch 138/200\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 145.0620\n",
      "Epoch 139/200\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 157.1614\n",
      "Epoch 140/200\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 183.4962\n",
      "Epoch 141/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 225.7541\n",
      "Epoch 142/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 138.8622\n",
      "Epoch 143/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 165.1523\n",
      "Epoch 144/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 155.6669\n",
      "Epoch 145/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 155.5755\n",
      "Epoch 146/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 211.8285\n",
      "Epoch 147/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 133.4096\n",
      "Epoch 148/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 182.4196\n",
      "Epoch 149/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 243.0591\n",
      "Epoch 150/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 181.5319\n",
      "Epoch 151/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 154.1196\n",
      "Epoch 152/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 171.0890\n",
      "Epoch 153/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 139.8911\n",
      "Epoch 154/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 275.2122\n",
      "Epoch 155/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 160.1360\n",
      "Epoch 156/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 170.5261\n",
      "Epoch 157/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 168.8614\n",
      "Epoch 158/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 144.9764\n",
      "Epoch 159/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 136.7454\n",
      "Epoch 160/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 127.0331\n",
      "Epoch 161/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 192.2361\n",
      "Epoch 162/200\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 172.0979\n",
      "Epoch 163/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 132.0441\n",
      "Epoch 164/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 148.5918\n",
      "Epoch 165/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 172.2387\n",
      "Epoch 166/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 118.5563\n",
      "Epoch 167/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 180.5663\n",
      "Epoch 168/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 135.5544\n",
      "Epoch 169/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 207.4591\n",
      "Epoch 170/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 149.6232\n",
      "Epoch 171/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 153.8232\n",
      "Epoch 172/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 145.8327\n",
      "Epoch 173/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 131.1722\n",
      "Epoch 174/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 200.5703\n",
      "Epoch 175/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 175.9767\n",
      "Epoch 176/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 239.5516\n",
      "Epoch 177/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 204.3606\n",
      "Epoch 178/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 143.0906\n",
      "Epoch 179/200\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 135.1763\n",
      "Epoch 180/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 142.6784\n",
      "Epoch 181/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 186.3221\n",
      "Epoch 182/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 169.3548\n",
      "Epoch 183/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 123.2206\n",
      "Epoch 184/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 148.6178\n",
      "Epoch 185/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 157.6857\n",
      "Epoch 186/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 165.1246\n",
      "Epoch 187/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 134.1420\n",
      "Epoch 188/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 126.8178\n",
      "Epoch 189/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 186.7885\n",
      "Epoch 190/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 190.8759\n",
      "Epoch 191/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 166.7124\n",
      "Epoch 192/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 122.7101\n",
      "Epoch 193/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 131.8300\n",
      "Epoch 194/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 122.3369\n",
      "Epoch 195/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 130.4745\n",
      "Epoch 196/200\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 146.1973\n",
      "Epoch 197/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 142.2481\n",
      "Epoch 198/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 109.1287\n",
      "Epoch 199/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 155.4987\n",
      "Epoch 200/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 160.0086\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 5999 and the array at index 1 has size 6000\n",
      "k_pneumoniae_driams_b_2000_20000Da_v2 (1).csv\n",
      "Archivo: k_pneumoniae_driams_b_2000_20000Da_v2 (1).csv Bacteria: Ceftriaxone\n",
      "Model: \"model_19\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_20 (InputLayer)          [(None, 6000)]       0           []                               \n",
      "                                                                                                  \n",
      " dense_36 (Dense)               (None, 16)           96016       ['input_20[0][0]']               \n",
      "                                                                                                  \n",
      " z_mean (Dense)                 (None, 2)            34          ['dense_36[0][0]']               \n",
      "                                                                                                  \n",
      " z_log_var (Dense)              (None, 2)            34          ['dense_36[0][0]']               \n",
      "                                                                                                  \n",
      " z (Lambda)                     (None, 2)            0           ['z_mean[0][0]',                 \n",
      "                                                                  'z_log_var[0][0]']              \n",
      "                                                                                                  \n",
      " dense_37 (Dense)               (None, 16)           48          ['z[0][0]']                      \n",
      "                                                                                                  \n",
      " dense_38 (Dense)               (None, 6000)         102000      ['dense_37[0][0]']               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_24 (TFOpL  (None, 2)           0           ['z_log_var[0][0]']              \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.math.square_12 (TFOpLambda)  (None, 2)           0           ['z_mean[0][0]']                 \n",
      "                                                                                                  \n",
      " tf.convert_to_tensor_12 (TFOpL  (None, 6000)        0           ['dense_38[0][0]']               \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.cast_12 (TFOpLambda)        (None, 6000)         0           ['input_20[0][0]']               \n",
      "                                                                                                  \n",
      " tf.math.subtract_24 (TFOpLambd  (None, 2)           0           ['tf.__operators__.add_24[0][0]',\n",
      " a)                                                               'tf.math.square_12[0][0]']      \n",
      "                                                                                                  \n",
      " tf.math.exp_12 (TFOpLambda)    (None, 2)            0           ['z_log_var[0][0]']              \n",
      "                                                                                                  \n",
      " tf.math.squared_difference_12   (None, 6000)        0           ['tf.convert_to_tensor_12[0][0]',\n",
      " (TFOpLambda)                                                     'tf.cast_12[0][0]']             \n",
      "                                                                                                  \n",
      " tf.math.subtract_25 (TFOpLambd  (None, 2)           0           ['tf.math.subtract_24[0][0]',    \n",
      " a)                                                               'tf.math.exp_12[0][0]']         \n",
      "                                                                                                  \n",
      " tf.math.reduce_mean_24 (TFOpLa  (None,)             0           ['tf.math.squared_difference_12[0\n",
      " mbda)                                                           ][0]']                           \n",
      "                                                                                                  \n",
      " tf.math.reduce_sum_12 (TFOpLam  (None,)             0           ['tf.math.subtract_25[0][0]']    \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " tf.math.multiply_24 (TFOpLambd  (None,)             0           ['tf.math.reduce_mean_24[0][0]'] \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.multiply_25 (TFOpLambd  (None,)             0           ['tf.math.reduce_sum_12[0][0]']  \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_25 (TFOpL  (None,)             0           ['tf.math.multiply_24[0][0]',    \n",
      " ambda)                                                           'tf.math.multiply_25[0][0]']    \n",
      "                                                                                                  \n",
      " tf.math.reduce_mean_25 (TFOpLa  ()                  0           ['tf.__operators__.add_25[0][0]']\n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " add_loss_12 (AddLoss)          ()                   0           ['tf.math.reduce_mean_25[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 198,132\n",
      "Trainable params: 198,132\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "1/1 [==============================] - 0s 393ms/step - loss: 862.7303\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 862.5479\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 856.9319\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 853.4750\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 851.8886\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 846.3053\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 846.0931\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 838.4082\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 834.9763\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 834.4030\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 833.5138\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 830.1956\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 818.6749\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 817.0307\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 833.2672\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 818.1568\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 831.5891\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 813.2979\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 812.9662\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 807.6062\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 797.8108\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 788.2254\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 794.8516\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 791.5952\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 810.8462\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 792.5001\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 823.0109\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 789.1996\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 772.5093\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 790.9826\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 765.8991\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 747.8543\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 760.8032\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 758.1221\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 751.0135\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 753.5658\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 776.2259\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 787.1364\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 731.4489\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 741.4896\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 765.2525\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 760.7382\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 745.8942\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 736.7413\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 711.4262\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 708.2610\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 747.3936\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 689.7382\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 724.6162\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 714.6686\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 694.9193\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 726.2187\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 703.8887\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 707.0201\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 746.8389\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 788.3595\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 684.8685\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 662.5825\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 690.2246\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 661.8260\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 653.5210\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 739.2125\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 687.8317\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 736.4189\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 660.9153\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 616.3867\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 637.0122\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 693.9316\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 653.5822\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 742.1013\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 658.1812\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 682.4734\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 633.1439\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 616.8746\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 632.9702\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 617.8657\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 684.6399\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 600.7864\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 511.5156\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 655.3099\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 606.3007\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 536.7242\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 600.0106\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 623.1616\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 613.8349\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 648.3566\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 709.8755\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 694.0021\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 522.7601\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 551.1872\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 595.2921\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 497.4691\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 656.9557\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 612.2858\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 585.0095\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 556.4835\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 526.9088\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 720.5688\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 573.6655\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 635.8470\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 553.8893\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 613.1513\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 584.6384\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 625.9514\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 500.4725\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 557.4497\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 562.2875\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 725.1064\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 540.9448\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 568.3000\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 556.1798\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 559.7380\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 575.5932\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 503.4977\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 581.9946\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 508.9721\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 593.8804\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 560.0825\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 498.7963\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 601.5458\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 497.0471\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 518.9457\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 603.0173\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 485.8772\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 589.1550\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 514.8491\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 627.8608\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 577.0598\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 562.2771\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 490.6710\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 558.1553\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 495.8374\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 535.1092\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 504.4845\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 547.5917\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 559.3169\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 688.9700\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 467.5910\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 564.2464\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 496.3108\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 478.9758\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 457.8855\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 450.2875\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 439.1070\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 543.3613\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 616.7954\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 462.6266\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 566.4069\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 531.8273\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 541.4827\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 525.5390\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 539.3824\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 508.7101\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 451.5034\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 491.7938\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 505.1783\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 474.9432\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 595.3934\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 542.4769\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 554.0582\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 562.1161\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 544.3224\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 499.3497\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 549.4601\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 550.5102\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 493.5254\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 550.5798\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 538.3889\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 487.1649\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 560.9514\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 563.7366\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 452.3411\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 572.7711\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 501.0330\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 523.2388\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 436.8438\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 555.5463\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 489.8862\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 437.3546\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 519.0430\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 437.2998\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 523.7791\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 409.2541\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 453.0820\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 460.0201\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 497.5397\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 498.5137\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 552.9002\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 550.0978\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 513.7964\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 484.1669\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 469.0951\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 504.6926\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 555.6961\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 532.4650\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 549.4672\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 517.4772\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 485.5024\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 491.3509\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 518.6355\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 5999 and the array at index 1 has size 6000\n",
      "s_aureus_driams_b_2000_20000Da_v2 (1).csv\n",
      "Archivo: s_aureus_driams_b_2000_20000Da_v2 (1).csv Bacteria: Clindamycin\n",
      "With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.\n",
      "s_aureus_driams_a_bin3_2000_20000Da.csv\n",
      "Archivo: s_aureus_driams_a_bin3_2000_20000Da.csv Bacteria: Clindamycin\n",
      "Model: \"model_21\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_22 (InputLayer)          [(None, 6000)]       0           []                               \n",
      "                                                                                                  \n",
      " dense_39 (Dense)               (None, 16)           96016       ['input_22[0][0]']               \n",
      "                                                                                                  \n",
      " z_mean (Dense)                 (None, 2)            34          ['dense_39[0][0]']               \n",
      "                                                                                                  \n",
      " z_log_var (Dense)              (None, 2)            34          ['dense_39[0][0]']               \n",
      "                                                                                                  \n",
      " z (Lambda)                     (None, 2)            0           ['z_mean[0][0]',                 \n",
      "                                                                  'z_log_var[0][0]']              \n",
      "                                                                                                  \n",
      " dense_40 (Dense)               (None, 16)           48          ['z[0][0]']                      \n",
      "                                                                                                  \n",
      " dense_41 (Dense)               (None, 6000)         102000      ['dense_40[0][0]']               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_26 (TFOpL  (None, 2)           0           ['z_log_var[0][0]']              \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.math.square_13 (TFOpLambda)  (None, 2)           0           ['z_mean[0][0]']                 \n",
      "                                                                                                  \n",
      " tf.convert_to_tensor_13 (TFOpL  (None, 6000)        0           ['dense_41[0][0]']               \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.cast_13 (TFOpLambda)        (None, 6000)         0           ['input_22[0][0]']               \n",
      "                                                                                                  \n",
      " tf.math.subtract_26 (TFOpLambd  (None, 2)           0           ['tf.__operators__.add_26[0][0]',\n",
      " a)                                                               'tf.math.square_13[0][0]']      \n",
      "                                                                                                  \n",
      " tf.math.exp_13 (TFOpLambda)    (None, 2)            0           ['z_log_var[0][0]']              \n",
      "                                                                                                  \n",
      " tf.math.squared_difference_13   (None, 6000)        0           ['tf.convert_to_tensor_13[0][0]',\n",
      " (TFOpLambda)                                                     'tf.cast_13[0][0]']             \n",
      "                                                                                                  \n",
      " tf.math.subtract_27 (TFOpLambd  (None, 2)           0           ['tf.math.subtract_26[0][0]',    \n",
      " a)                                                               'tf.math.exp_13[0][0]']         \n",
      "                                                                                                  \n",
      " tf.math.reduce_mean_26 (TFOpLa  (None,)             0           ['tf.math.squared_difference_13[0\n",
      " mbda)                                                           ][0]']                           \n",
      "                                                                                                  \n",
      " tf.math.reduce_sum_13 (TFOpLam  (None,)             0           ['tf.math.subtract_27[0][0]']    \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " tf.math.multiply_26 (TFOpLambd  (None,)             0           ['tf.math.reduce_mean_26[0][0]'] \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.multiply_27 (TFOpLambd  (None,)             0           ['tf.math.reduce_sum_13[0][0]']  \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_27 (TFOpL  (None,)             0           ['tf.math.multiply_26[0][0]',    \n",
      " ambda)                                                           'tf.math.multiply_27[0][0]']    \n",
      "                                                                                                  \n",
      " tf.math.reduce_mean_27 (TFOpLa  ()                  0           ['tf.__operators__.add_27[0][0]']\n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " add_loss_13 (AddLoss)          ()                   0           ['tf.math.reduce_mean_27[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 198,132\n",
      "Trainable params: 198,132\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 972.0109\n",
      "Epoch 2/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 684.4464\n",
      "Epoch 3/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 480.9643\n",
      "Epoch 4/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 374.9253\n",
      "Epoch 5/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 310.0149\n",
      "Epoch 6/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 284.7785\n",
      "Epoch 7/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 242.0801\n",
      "Epoch 8/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 223.0273\n",
      "Epoch 9/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 212.3215\n",
      "Epoch 10/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 196.5235\n",
      "Epoch 11/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 169.3172\n",
      "Epoch 12/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 161.2318\n",
      "Epoch 13/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 162.7128\n",
      "Epoch 14/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 133.4549\n",
      "Epoch 15/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 134.6245\n",
      "Epoch 16/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 135.8004\n",
      "Epoch 17/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 125.0861\n",
      "Epoch 18/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 115.2605\n",
      "Epoch 19/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 112.9356\n",
      "Epoch 20/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 108.3671\n",
      "Epoch 21/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 103.9711\n",
      "Epoch 22/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 101.1198\n",
      "Epoch 23/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 98.4694\n",
      "Epoch 24/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 102.1967\n",
      "Epoch 25/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 100.0348\n",
      "Epoch 26/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 99.3828\n",
      "Epoch 27/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 102.9071\n",
      "Epoch 28/200\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 69.5862"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 254\u001b[0m\n\u001b[1;32m    235\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m, random_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, stratify\u001b[38;5;241m=\u001b[39my)\n\u001b[1;32m    237\u001b[0m \u001b[38;5;66;03m#resultado sin oversampling\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;66;03m#model,tipo_entrenamiento,cm,y_pred,X_test_reshaped,X_train_reshaped = entrenamiento_base(X_train, X_test, y_train, y_test)\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;66;03m#inscripcion_resultados(model,archivo,bacteria,cm,y_test, y_pred,tipo_entrenamiento,X_test_reshaped,X_train_reshaped)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    252\u001b[0m \n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m#resultado con VAE\u001b[39;00m\n\u001b[0;32m--> 254\u001b[0m model,tipo_entrenamiento,cm,y_pred,X_test_reshaped,X_train_reshaped \u001b[38;5;241m=\u001b[39m \u001b[43mAplicar_VAE\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_bacteria\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbacteria\u001b[49m\u001b[43m,\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m inscripcion_resultados(model,archivo,bacteria,cm,y_test, y_pred,tipo_entrenamiento,X_test_reshaped,X_train_reshaped)\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m# Liberar memoria\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[23], line 161\u001b[0m, in \u001b[0;36mAplicar_VAE\u001b[0;34m(df_bacteria, bacteria, X_train, X_test, y_train, y_test)\u001b[0m\n\u001b[1;32m    158\u001b[0m vae\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    160\u001b[0m vae\u001b[38;5;241m.\u001b[39msummary()\n\u001b[0;32m--> 161\u001b[0m \u001b[43mvae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_minority_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_minority_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;66;03m# Construir el generador (Decoder independiente)\u001b[39;00m\n\u001b[1;32m    164\u001b[0m decoder_input \u001b[38;5;241m=\u001b[39m Input(shape\u001b[38;5;241m=\u001b[39m(latent_dim,))\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/keras/engine/training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1562\u001b[0m ):\n\u001b[1;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2494\u001b[0m   (graph_function,\n\u001b[1;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1865\u001b[0m     args,\n\u001b[1;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1867\u001b[0m     executing_eagerly)\n\u001b[1;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,classification_report,ConfusionMatrixDisplay, balanced_accuracy_score\n",
    "#from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler, MinMaxScaler, MaxAbsScaler, RobustScaler, QuantileTransformer, PowerTransformer\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import regularizers\n",
    "from keras.optimizers import Adam\n",
    "#from keras.backend import expand_dims\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from keras.models import Sequential,Model\n",
    "from keras.constraints import MaxNorm\n",
    "from keras.layers import Activation, Dense, Conv1D, Flatten, MaxPooling1D, Dropout, BatchNormalization, SpatialDropout1D,Lambda,Input\n",
    "import os\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import gc\n",
    "from tensorflow.keras.losses import mse\n",
    "\n",
    "\n",
    "METRICS = [\n",
    "      keras.metrics.TruePositives(name='tp'),\n",
    "      keras.metrics.FalsePositives(name='fp'),\n",
    "      keras.metrics.TrueNegatives(name='tn'),\n",
    "      keras.metrics.FalseNegatives(name='fn'),\n",
    "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall'),\n",
    "      keras.metrics.AUC(name='auc'),\n",
    "      keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "]\n",
    "\n",
    "def Crear_modelo(X_train_reshaped,y_train):\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, min_lr=0.000001)\n",
    "    early_st = EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)\n",
    "\n",
    "    n_timesteps = X_train_reshaped.shape[1] #\n",
    "    n_features  = X_train_reshaped.shape[2] #\n",
    "\n",
    "    model = Sequential(name=\"Modelo_s_aureus_ciprofloxacin\")\n",
    "    init_mode = 'normal'\n",
    "    model.add(Conv1D(filters=(64), kernel_size=(17), input_shape = (n_timesteps,n_features), name='Conv_1'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2, name=\"MaxPooling1D_1\"))\n",
    "\n",
    "    model.add(Conv1D(filters=(128), kernel_size=(9),kernel_initializer=init_mode, kernel_regularizer=regularizers.l2(0.0001),  name='Conv_2'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2, name=\"MaxPooling1D_2\"))\n",
    "\n",
    "    model.add(Conv1D(filters=(256), kernel_size=(5),kernel_initializer=init_mode,kernel_regularizer=regularizers.l2(0.0001),   name='Conv_3'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2, name=\"MaxPooling1D_3\"))\n",
    "\n",
    "    model.add(Conv1D(filters=(256), kernel_size=(5),kernel_initializer=init_mode, kernel_regularizer=regularizers.l2(0.0001),   name='Conv_4'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2, name=\"MaxPooling1D_4\"))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.65))\n",
    "    model.add(Dense(256, activation='relu',kernel_initializer=init_mode, kernel_regularizer=regularizers.l2(0.0001), name=\"fully_connected_0\"))\n",
    "    model.add(Dense(64, activation='relu',kernel_initializer=init_mode, kernel_regularizer=regularizers.l2(0.0001), name=\"fully_connected_1\"))\n",
    "    model.add(Dense(64, activation='relu',kernel_initializer=init_mode, kernel_regularizer=regularizers.l2(0.0001),  name=\"fully_connected_2\"))\n",
    "    model.add(Dense(n_features, activation='sigmoid', name=\"OUT_Layer\"))\n",
    "\n",
    "    model.compile(optimizer = Adam(learning_rate=0.0001), loss = 'binary_crossentropy',  metrics=METRICS)\n",
    "    model.summary()\n",
    "    history = model.fit(X_train_reshaped, y_train, epochs=100, batch_size=10, verbose=1, validation_split=0.1, callbacks=[reduce_lr,early_st])\n",
    "    return model\n",
    "\n",
    "def normalizacion(X_train, X_test):\n",
    "    scaler=Normalizer(norm='max')\n",
    "    sc_X = scaler\n",
    "    X_train = sc_X.fit_transform(X_train)\n",
    "    X_test = sc_X.transform(X_test)\n",
    "\n",
    "    sample_size = X_train.shape[0] # numero de muestras en el set de datos\n",
    "    time_steps  = X_train.shape[1] # numero de atributos en el set de datos\n",
    "    input_dimension = 1            #\n",
    "\n",
    "    X_train_reshaped = X_train.reshape(sample_size,time_steps,input_dimension)\n",
    "    X_test_reshaped = X_test.reshape(X_test.shape[0],X_test.shape[1],1)\n",
    "    return X_train_reshaped,X_test_reshaped\n",
    "\n",
    "\n",
    "def entrenamiento_base(X_train, X_test, y_train, y_test):\n",
    "    X_train_reshaped,X_test_reshaped = normalizacion(X_train, X_test)\n",
    "    \n",
    "    model = Crear_modelo(X_train_reshaped,y_train)\n",
    "\n",
    "    y_pred  = model.predict(X_test_reshaped)\n",
    "    y_pred = (y_pred>0.5)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    #model,tipo_entrenamiento,cm,y_pred,X_test_reshaped,X_train_reshaped\n",
    "    return model,'Entrenamiento base',cm,y_pred,X_test_reshaped,X_train_reshaped\n",
    "\n",
    "def Aplicar_Smote(X_train, X_test, y_train, y_test):\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_resampled_smote, y_resampled_smote = smote.fit_resample(X_train, y_train)\n",
    "    X_train_reshaped,X_test_reshaped = normalizacion(X_resampled_smote, X_test)\n",
    "\n",
    "    model = Crear_modelo(X_train_reshaped,y_resampled_smote)\n",
    "    y_pred  = model.predict(X_test_reshaped)\n",
    "\n",
    "    y_pred = (y_pred>0.5)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    #model,tipo_entrenamiento,cm,y_pred,X_test_reshaped,X_train_reshaped\n",
    "    return model,'Entrenamiento Smote',cm,y_pred,X_test_reshaped,X_train_reshaped\n",
    "\n",
    "def Aplicar_VAE(df_bacteria,bacteria,X_train, X_test, y_train, y_test):\n",
    "    minority_class = df_bacteria[df_bacteria[bacteria] == 1].drop(columns=[bacteria])\n",
    "    scaler = MinMaxScaler()\n",
    "    X_minority_scaled = scaler.fit_transform(minority_class)\n",
    "    # Dimensiones\n",
    "    input_dim = X_minority_scaled.shape[1]\n",
    "    latent_dim = 2  # Espacio latente\n",
    "\n",
    "    # Encoder\n",
    "    inputs = Input(shape=(input_dim,))\n",
    "    hidden = Dense(16, activation='relu')(inputs)\n",
    "    z_mean = Dense(latent_dim, name='z_mean')(hidden)\n",
    "    z_log_var = Dense(latent_dim, name='z_log_var')(hidden)\n",
    "\n",
    "    # Sampling\n",
    "    def sampling(args):\n",
    "        z_mean, z_log_var = args\n",
    "        epsilon = tf.random.normal(shape=(tf.shape(z_mean)[0], latent_dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "    z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "\n",
    "    # Decoder\n",
    "    decoder_hidden = Dense(16, activation='relu')\n",
    "    decoder_output = Dense(input_dim, activation='sigmoid')\n",
    "\n",
    "    hidden_decoded = decoder_hidden(z)\n",
    "    outputs = decoder_output(hidden_decoded)\n",
    "\n",
    "    # Modelo VAE\n",
    "    vae = Model(inputs, outputs)\n",
    "\n",
    "    # Prdida personalizada\n",
    "    reconstruction_loss = mse(inputs, outputs)\n",
    "    reconstruction_loss *= input_dim\n",
    "    kl_loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n",
    "    kl_loss = tf.reduce_sum(kl_loss, axis=-1)\n",
    "    kl_loss *= -0.5\n",
    "    vae_loss = tf.reduce_mean(reconstruction_loss + kl_loss)\n",
    "    vae.add_loss(vae_loss)\n",
    "    vae.compile(optimizer='adam')\n",
    "\n",
    "    vae.summary()\n",
    "    vae.fit(X_minority_scaled, X_minority_scaled, epochs=200, batch_size=32, verbose=1)\n",
    "\n",
    "    # Construir el generador (Decoder independiente)\n",
    "    decoder_input = Input(shape=(latent_dim,))\n",
    "    hidden_decoded_2 = decoder_hidden(decoder_input)\n",
    "    output_decoded = decoder_output(hidden_decoded_2)\n",
    "    generator = Model(decoder_input, output_decoded)\n",
    "\n",
    "    # Generar datos sintticos\n",
    "    num_samples = pd.Series(y_train).value_counts()[0]-pd.Series(y_train).value_counts()[1]\n",
    "    latent_points = np.random.normal(size=(num_samples, latent_dim))\n",
    "    synthetic_data = generator.predict(latent_points)\n",
    "\n",
    "    # Escalar de vuelta a los valores originales\n",
    "    synthetic_data_original = scaler.inverse_transform(synthetic_data)\n",
    "    X_train_Vae = np.concatenate([X_train, synthetic_data_original])\n",
    "    y_train_Vae = np.concatenate([y_train, np.ones(num_samples)])\n",
    "\n",
    "    \n",
    "    \"\"\" \n",
    "    X_train_reshaped,X_test_reshaped = normalizacion(X_train_Vae, X_test)\n",
    "    \n",
    "    model = Crear_modelo(X_train_reshaped,y_train_Vae)\n",
    "    y_pred  = model.predict(X_test_reshaped)\n",
    "\n",
    "    y_pred = (y_pred>0.5)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    #model,tipo_entrenamiento,cm,y_pred,X_test_reshaped,X_train_reshaped\n",
    "    return model,'Entrenamiento Vae',cm,y_pred,X_test_reshaped,X_train_reshaped \"\"\"\n",
    "\n",
    "\n",
    "def columnas_bacterias_fun(df):\n",
    "    vocales = ['a','e','i','o','u']\n",
    "    columnas_bacterias = []\n",
    "    for i in vocales:\n",
    "        for j in df.columns:\n",
    "            if i in j:\n",
    "                columnas_bacterias.append(j)\n",
    "    columnas_bacterias = list(set(columnas_bacterias))\n",
    "    return columnas_bacterias\n",
    "\n",
    "def inscripcion_resultados(model,archivo,bacteria,cm,y_test, y_pred,tipo_entrenamiento,X_test_reshaped,X_train_reshaped):\n",
    "    with open('resultados/resultados.txt', 'a') as archivo_:\n",
    "        # Redirige la salida estndar al archivo\n",
    "        print('-----------------------------------------------------\\n\\n','nombre de archivo:', archivo, '\\nBacteria:', bacteria, '\\n\\nconfusion_matrix:\\n', cm,\"\\n\\nTipo de entrenamiento:\",tipo_entrenamiento, file=archivo_)\n",
    "        target_names=[\"0\",\"1\"]\n",
    "        print('\\n\\n',classification_report(y_test, y_pred, target_names=target_names), file=archivo_)\n",
    "\n",
    "        train_predictions_baseline = model.predict(X_train_reshaped, batch_size=10)\n",
    "        test_predictions_baseline = model.predict(X_test_reshaped, batch_size=10)\n",
    "        print('\\n\\n')\n",
    "        baseline_results = model.evaluate(X_test_reshaped, y_test, verbose=0)\n",
    "        for name, value in zip(model.metrics_names, baseline_results):\n",
    "            print(name, ': ', value, file=archivo_)  \n",
    "\n",
    "\n",
    "\n",
    "files_list = os.listdir('SetDatos/')\n",
    "for archivo in files_list:\n",
    "    print(archivo)\n",
    "    df = pd.read_csv('SetDatos/'+archivo)\n",
    "    df = df.drop(columns=['code','species'])\n",
    "    df.dropna(axis=0, how=\"any\", inplace=True)\n",
    "    columnas_bacterias = columnas_bacterias_fun(df)\n",
    "    for bacteria in columnas_bacterias:\n",
    "\n",
    "        try:\n",
    "            print('Archivo:',archivo,'Bacteria:',bacteria)\n",
    "            columnas_bacterias_sin_bacteria = [b for b in columnas_bacterias if b != bacteria]\n",
    "            df_bacteria = df.drop(columns = columnas_bacterias_sin_bacteria)\n",
    "            bacteria = df_bacteria.columns[-1]\n",
    "            X = df_bacteria.iloc[:, 0:-2].values  # variables independientes (espectros de masa)\n",
    "            y = df_bacteria.iloc[:, -1].values    # variable dependientes (resistencia a ciprofloxacin)\n",
    "            X = np.asarray(X).astype(np.float32)\n",
    "            y = np.asarray(y).astype(np.float32)\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0, stratify=y)\n",
    " \n",
    "            #resultado sin oversampling\n",
    "            #model,tipo_entrenamiento,cm,y_pred,X_test_reshaped,X_train_reshaped = entrenamiento_base(X_train, X_test, y_train, y_test)\n",
    "            #inscripcion_resultados(model,archivo,bacteria,cm,y_test, y_pred,tipo_entrenamiento,X_test_reshaped,X_train_reshaped)\n",
    "        \n",
    "            # Liberar memoria\n",
    "            #del model, tipo_entrenamiento, y_pred, cm, X_test_reshaped, X_train_reshaped\n",
    "            #gc.collect()  # Forzar recoleccin de basura\n",
    "            \n",
    "            #resultado con smote\n",
    "            #model,tipo_entrenamiento,cm,y_pred,X_test_reshaped,X_train_reshaped = Aplicar_Smote(X_train, X_test, y_train, y_test)\n",
    "            #inscripcion_resultados(model,archivo,bacteria,cm,y_test, y_pred,tipo_entrenamiento,X_test_reshaped,X_train_reshaped)\n",
    "\n",
    "            # Liberar memoria\n",
    "            #del model, tipo_entrenamiento, y_pred, cm, X_test_reshaped, X_train_reshaped\n",
    "            #gc.collect()  # Forzar recoleccin de basura\n",
    "            \n",
    "            #resultado con VAE\n",
    "            model,tipo_entrenamiento,cm,y_pred,X_test_reshaped,X_train_reshaped = Aplicar_VAE(df_bacteria,bacteria,X_train, X_test, y_train, y_test)\n",
    "            inscripcion_resultados(model,archivo,bacteria,cm,y_test, y_pred,tipo_entrenamiento,X_test_reshaped,X_train_reshaped)\n",
    "            \n",
    "            # Liberar memoria\n",
    "            del model, tipo_entrenamiento, y_pred, cm, X_test_reshaped, X_train_reshaped\n",
    "            gc.collect()  # Forzar recoleccin de basura\n",
    "\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "            with open('resultados/resultados.txt', 'a') as archivo_:\n",
    "                print(\"Error:\",e,file = archivo_)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
